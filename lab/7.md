
**Chapter 6 (Optional)**
# Using GitHub Copilot SDK with Local AI Models

**‚è±Ô∏è Estimated Time**: 30-45 minutes  
**üìã Prerequisites**: Completed Chapters 1-5

---

## Overview

In this optional chapter, you'll learn how to leverage the **GitHub Copilot SDK** (2024-2025 release) with **local AI models** to run your Pizza Ordering Agent entirely on your local machine.

This approach provides:
- üîí **Privacy**: Your code and data never leave your machine
- üí∞ **Cost savings**: No cloud compute charges
- ‚ö° **Low latency**: Fast local execution
- üì¥ **Offline capability**: Work without internet connectivity

## What You'll Learn

By the end of this chapter, you'll be able to:
1. Set up Microsoft Foundry Local with the AI Toolkit
2. Configure local AI models (Phi-3, Qwen) in Visual Studio Code
3. Run your Pizza Ordering Agent using local models
4. Build a custom GitHub Copilot Extension for the pizza domain
5. Implement a hybrid architecture (cloud + local models)

## Prerequisites

Before starting this chapter, ensure you have:
- ‚úÖ Completed Lab Chapters 1-5
- ‚úÖ Visual Studio Code installed
- ‚úÖ Python 3.9+ or Node.js 18+
- ‚úÖ At least 8GB RAM (16GB recommended)
- ‚úÖ 10GB free disk space for model downloads

---

## Part 1: Setting Up Local AI Models

### Step 1.1: Install AI Toolkit Extension

1. Open Visual Studio Code
2. Go to Extensions (Ctrl+Shift+X or Cmd+Shift+X)
3. Search for **"AI Toolkit for Visual Studio Code"**
4. Click **Install** (Publisher: Microsoft)

Alternatively, install via command line:

```bash
code --install-extension ms-windows-ai-studio.windows-ai-studio
```

### Step 1.2: Install Microsoft Foundry Local

Open a terminal and run:

```bash
# Install Foundry Local
pip install azure-ai-foundry-local

# Verify installation
foundry-local --version
```

### Step 1.3: Start Foundry Local Server

```bash
# Start the local model server
foundry-local serve --host 0.0.0.0 --port 8000

# You should see output like:
# ‚úì Foundry Local server started
# ‚úì Listening on http://localhost:8000
# ‚úì Ready to serve models
```

Keep this terminal open - the server needs to run continuously.

### Step 1.4: Configure AI Toolkit

1. In VS Code, open the Command Palette (Ctrl+Shift+P / Cmd+Shift+P)
2. Type: **"AI Toolkit: Select Model"**
3. Choose: **"Foundry Local via AI Toolkit"**
4. Select model: **"Phi-3-mini"** (efficient, 2-4GB RAM)

The first time you select a model, it will download automatically (~2-3GB).

---

## Part 2: Running Your Agent with Local Models

### Step 2.1: Create a Local Agent Configuration

Create a new file: **agent_local.py**

```python
import os
from azure.ai.projects import AIProjectClient
from azure.identity import DefaultAzureCredential
from azure.ai.agents.models import MessageRole

# Configure local model endpoint
LOCAL_ENDPOINT = "http://localhost:8000/v1/chat/completions"
LOCAL_MODEL = "phi-3-mini"

def create_local_agent():
    """Create Pizza Ordering Agent using local Phi-3 model"""
    
    # Override endpoint to use local server
    project_client = AIProjectClient(
        credential=DefaultAzureCredential(),
        endpoint=LOCAL_ENDPOINT
    )
    
    # Create agent with local model
    agent = project_client.agents.create_agent(
        model=LOCAL_MODEL,
        name="PizzaBot Local",
        instructions="""You are a helpful pizza ordering assistant for Contoso Pizza.
        
        You can help customers:
        - Browse our pizza menu
        - Get pizza recommendations
        - Calculate pizza sizes for groups
        - Place orders
        
        Always be friendly, helpful, and pizza-enthusiastic! üçï
        """,
    )
    
    return project_client, agent

def chat_with_local_agent(project_client, agent, message: str):
    """Send a message to the local agent"""
    
    # Create a thread
    thread = project_client.agents.create_thread()
    
    # Add user message
    project_client.agents.create_message(
        thread_id=thread.id,
        role=MessageRole.USER,
        content=message
    )
    
    # Run the agent
    run = project_client.agents.create_run(
        thread_id=thread.id,
        agent_id=agent.id
    )
    
    # Wait for completion
    while run.status in ["queued", "in_progress"]:
        time.sleep(1)
        run = project_client.agents.get_run(
            thread_id=thread.id,
            run_id=run.id
        )
    
    # Get messages
    messages = project_client.agents.get_messages(thread_id=thread.id)
    
    # Return last assistant message
    for message in messages:
        if message.role == MessageRole.ASSISTANT:
            return message.content[0].text.value
    
    return "No response"

if __name__ == "__main__":
    print("üçï Starting PizzaBot with local Phi-3 model...")
    
    # Create local agent
    client, agent = create_local_agent()
    print(f"‚úì Agent created: {agent.name}")
    
    # Test the agent
    response = chat_with_local_agent(
        client, 
        agent, 
        "What pizzas do you recommend for a vegetarian?"
    )
    
    print(f"\nü§ñ Agent response:\n{response}")
```

### Step 2.2: Test Your Local Agent

Run the script:

```bash
python agent_local.py
```

**Expected output**:
```
üçï Starting PizzaBot with local Phi-3 model...
‚úì Agent created: PizzaBot Local

ü§ñ Agent response:
For vegetarians, I recommend our delicious Margherita pizza with fresh 
mozzarella and basil, or our Veggie Supreme loaded with bell peppers, 
mushrooms, olives, and onions! Both are customer favorites! üçï
```

### Step 2.3: Compare Performance

Let's measure the performance difference:

```python
import time

def benchmark_agent(client, agent, test_messages):
    """Benchmark agent response times"""
    
    results = []
    
    for message in test_messages:
        start_time = time.time()
        response = chat_with_local_agent(client, agent, message)
        end_time = time.time()
        
        results.append({
            "message": message,
            "response_time": end_time - start_time,
            "response_length": len(response)
        })
    
    return results

# Test messages
test_messages = [
    "What's on a Margherita pizza?",
    "How many pizzas for 8 people?",
    "Do you have gluten-free options?"
]

# Run benchmark
print("\nüìä Performance Benchmark:")
results = benchmark_agent(client, agent, test_messages)

for i, result in enumerate(results, 1):
    print(f"\n{i}. {result['message']}")
    print(f"   Response time: {result['response_time']:.2f}s")
    print(f"   Response length: {result['response_length']} chars")
```

---

## Part 3: Building a Copilot Extension

### Step 3.1: Install GitHub Copilot SDK

```bash
# For Python
pip install github-copilot-sdk

# For Node.js (if you prefer JavaScript)
npm install @github/copilot-sdk
```

### Step 3.2: Create a Pizza Domain Copilot Extension

Create **copilot_pizza_extension.py**:

```python
from github_copilot import CopilotClient, Skillset

# Initialize Copilot client with local endpoint
client = CopilotClient(
    model_endpoint="http://localhost:8000/v1/chat/completions",
    model_name="phi-3-mini"
)

# Define pizza-specific skillset
pizza_skillset = Skillset(
    name="contoso-pizza",
    description="Pizza ordering and menu assistance for Contoso Pizza",
    tools=[
        {
            "name": "get_pizza_menu",
            "description": "Get the current pizza menu with prices",
            "parameters": {
                "type": "object",
                "properties": {
                    "category": {
                        "type": "string",
                        "enum": ["all", "vegetarian", "meat-lovers", "specialty"],
                        "description": "Pizza category to filter by"
                    }
                }
            }
        },
        {
            "name": "calculate_pizza_size",
            "description": "Calculate how many pizzas needed for a group",
            "parameters": {
                "type": "object",
                "properties": {
                    "people": {
                        "type": "integer",
                        "description": "Number of people to feed"
                    },
                    "slices_per_person": {
                        "type": "integer",
                        "default": 3,
                        "description": "Average slices per person"
                    }
                },
                "required": ["people"]
            }
        },
        {
            "name": "recommend_pizza",
            "description": "Get personalized pizza recommendations",
            "parameters": {
                "type": "object",
                "properties": {
                    "preferences": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Dietary preferences or favorite toppings"
                    }
                }
            }
        }
    ]
)

# Register the skillset
client.register_skillset(pizza_skillset)

# Test the extension
print("üçï Contoso Pizza Copilot Extension registered!")
print(f"‚úì Skillset: {pizza_skillset.name}")
print(f"‚úì Tools available: {len(pizza_skillset.tools)}")

# Example: Use the extension for code completion
completion = client.complete(
    prompt="# Calculate pizzas for 15 people\nnum_pizzas = ",
    context="Working on Contoso Pizza ordering system"
)

print(f"\nüí° Code suggestion:\n{completion}")
```

### Step 3.3: Test the Extension

```bash
python copilot_pizza_extension.py
```

---

## Part 4: Hybrid Architecture (Cloud + Local)

For production use, you might want to combine cloud and local models.

### Step 4.1: Create a Hybrid Agent

Create **agent_hybrid.py**:

```python
import os
from typing import Literal

class HybridPizzaBot:
    """Pizza agent that uses both cloud and local models"""
    
    def __init__(self):
        # Cloud agent for complex queries
        self.cloud_agent = self._create_cloud_agent()
        
        # Local agent for simple queries
        self.local_agent = self._create_local_agent()
        
        self.query_count = {"cloud": 0, "local": 0}
    
    def _create_cloud_agent(self):
        """Create cloud-based agent (Azure)"""
        # Use your existing agent from previous chapters
        pass
    
    def _create_local_agent(self):
        """Create local agent (Foundry Local)"""
        # Use the local agent from Part 2
        pass
    
    def classify_query(self, query: str) -> Literal["simple", "complex"]:
        """Determine if query is simple or complex"""
        
        # Simple heuristics
        complex_keywords = [
            "calculate", "recommend", "compare", "analyze",
            "multiple", "detailed", "explain"
        ]
        
        query_lower = query.lower()
        
        # Long queries are usually complex
        if len(query.split()) > 15:
            return "complex"
        
        # Check for complex keywords
        if any(keyword in query_lower for keyword in complex_keywords):
            return "complex"
        
        return "simple"
    
    def process_query(self, query: str) -> str:
        """Process query using appropriate agent"""
        
        query_type = self.classify_query(query)
        
        if query_type == "simple":
            print(f"üè† Using local model (faster, private)")
            self.query_count["local"] += 1
            return self._process_with_local(query)
        else:
            print(f"‚òÅÔ∏è  Using cloud model (more capable)")
            self.query_count["cloud"] += 1
            return self._process_with_cloud(query)
    
    def _process_with_local(self, query: str) -> str:
        """Process with local agent"""
        return chat_with_local_agent(
            self.local_client, 
            self.local_agent, 
            query
        )
    
    def _process_with_cloud(self, query: str) -> str:
        """Process with cloud agent"""
        # Use your existing cloud agent logic
        pass
    
    def get_stats(self):
        """Get usage statistics"""
        total = sum(self.query_count.values())
        if total == 0:
            return "No queries processed yet"
        
        local_pct = (self.query_count["local"] / total) * 100
        cloud_pct = (self.query_count["cloud"] / total) * 100
        
        return f"""
üìä Usage Statistics:
   Local queries: {self.query_count['local']} ({local_pct:.1f}%)
   Cloud queries: {self.query_count['cloud']} ({cloud_pct:.1f}%)
   
üí∞ Estimated savings: {local_pct:.0f}% of queries processed locally
        """

# Example usage
if __name__ == "__main__":
    bot = HybridPizzaBot()
    
    # Test different query types
    queries = [
        "What's on a Margherita?",  # Simple ‚Üí Local
        "Compare all meat pizzas and recommend the best one for a party of 20",  # Complex ‚Üí Cloud
        "Do you have pepperoni?",  # Simple ‚Üí Local
    ]
    
    for query in queries:
        print(f"\n‚ùì Query: {query}")
        response = bot.process_query(query)
        print(f"ü§ñ Response: {response}\n")
    
    print(bot.get_stats())
```

---

## Part 5: Best Practices

### Performance Optimization

```python
# 1. Use smaller context windows for local models
max_context = 2048  # vs 8192 for cloud

# 2. Enable GPU acceleration (if available)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# 3. Cache model weights
model_cache_dir = os.path.expanduser("~/.cache/foundry-local")
os.environ["FOUNDRY_LOCAL_CACHE"] = model_cache_dir
```

### Security Best Practices

```python
# Never hardcode endpoints
from dotenv import load_dotenv
load_dotenv()

LOCAL_ENDPOINT = os.getenv("FOUNDRY_LOCAL_ENDPOINT", "http://localhost:8000")
```

### Model Selection Guide

| Model | RAM Required | Best For | Speed |
|-------|--------------|----------|-------|
| Phi-3-mini | 2-4 GB | Quick queries, completions | ‚ö°‚ö°‚ö° |
| Phi-3-medium | 8-16 GB | Balanced performance | ‚ö°‚ö° |
| Qwen-Coder | 8-16 GB | Code-specific tasks | ‚ö°‚ö° |
| CodeLlama | 16+ GB | Large context code | ‚ö° |

---

## Challenges üèÜ

Try these optional challenges to deepen your learning:

### Challenge 1: Model Comparison
Compare response quality between Phi-3, Qwen, and your cloud model for the same pizza queries.

### Challenge 2: Fine-Tuning
Fine-tune a local model specifically for Contoso Pizza's menu and terminology.

### Challenge 3: Offline Mode
Implement a fully offline pizza ordering system that works without any internet connection.

### Challenge 4: Performance Dashboard
Build a dashboard that shows real-time performance metrics (response time, model usage, cost savings).

---

## Troubleshooting

### Issue: "Cannot connect to Foundry Local"
**Solution**: 
```bash
# Check if server is running
curl http://localhost:8000/health

# Restart server
foundry-local serve --host 0.0.0.0 --port 8000
```

### Issue: "Out of memory"
**Solution**: Switch to a smaller model (Phi-3-mini) or close other applications.

### Issue: "Model download failed"
**Solution**: Check internet connection and available disk space (need 10GB+).

---

## Summary

Congratulations! üéâ You've learned how to:

- ‚úÖ Set up Microsoft Foundry Local with local AI models
- ‚úÖ Run your Pizza Ordering Agent entirely on your machine
- ‚úÖ Build custom GitHub Copilot Extensions
- ‚úÖ Implement hybrid cloud + local architectures
- ‚úÖ Optimize performance and reduce costs

### Key Takeaways

- **Local models** provide privacy, cost savings, and offline capability
- **Hybrid architectures** balance performance and cost
- **GitHub Copilot SDK** enables custom domain-specific extensions
- **Microsoft Foundry Local** makes local AI development accessible

### Next Steps

- Explore the comprehensive guide: [docs/GITHUB_COPILOT_SDK_LOCAL_MODELS.md](../docs/GITHUB_COPILOT_SDK_LOCAL_MODELS.md)
- Join the discussion: [Microsoft Foundry Discord](https://aka.ms/MicrosoftFoundry-Ignite25)
- Try the [Continue.dev](https://continue.dev) extension for VS Code
- Experiment with [Ollama](https://ollama.com) as an alternative

---

**Want to learn more?** Check out the [full documentation](../docs/GITHUB_COPILOT_SDK_LOCAL_MODELS.md) for advanced topics, security considerations, and more examples.

---

**üçï Happy coding with local AI!**
